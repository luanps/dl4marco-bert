{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luanps/dl4marco-bert/blob/master/monoBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B9HlalBlPtT"
      },
      "source": [
        "**This colab was mainly copied from the original [BERT finetuning tasks in 5 minutes with Cloud TPU](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)**\n",
        "\n",
        "This Colab demonstates using a free Colab Cloud TPU to fine-tune passage re-ranker built on top of pretrained BERT models.\n",
        "\n",
        "Note: You will need a GCP (Google Compute Engine) account and a GCS (Google Cloud Storage) bucket for this Colab to run.\n",
        "\n",
        "Please follow the Google Cloud TPU quickstart for how to create GCP account and GCS bucket. You have $300 free credit to get started with any GCP product. You can learn more about Cloud TPU at https://cloud.google.com/tpu/docs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5N7IU7zl1UI"
      },
      "source": [
        "**Firstly**, we need to set up Colab TPU running environment, verify a TPU device is succesfully connected and upload credentials to TPU for GCS bucket usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET8rn1TNhrlc",
        "outputId": "8603b24e-f103-4e8f-a075-8442e83cfab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "TPU address is grpc://10.109.88.82:8470\n",
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&state=fBCCCt9956l8YGt69NKAddXwcjEsjN&prompt=consent&access_type=offline&code_challenge=J4mR0HwmZaMRDtkGW9u4JhgN1EaPGmAGGw8f2tOufNQ&code_challenge_method=S256\n",
            "\n",
            "Enter verification code: 4/1AX4XfWg2iu0h8bCKuJUEWWF4rWZzCgP_AENYlYY2EspwgRMfdccm8ojuNo0\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 2969983438738095363),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 3557507423780977462),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 12641580651787681120),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 10622143770257505920),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 1751978237863363802),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 7636394890270047714),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9671102101800674610),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11078864006197223864),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13783986827923181906),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 3125078371328146750),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 7227245709239882465)]\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZDNgAEdmRUR"
      },
      "source": [
        "**Secondly**, prepare and import BERT modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USzW4PWnmQqW",
        "outputId": "7d2825ba-1598-4bf0-9284-149c42434da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dl4marco-bert'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 170 (delta 1), reused 1 (delta 0), pack-reused 163\u001b[K\n",
            "Receiving objects: 100% (170/170), 166.40 KiB | 2.25 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "!test -d bert_repo || git clone https://github.com/luanps/dl4marco-bert dl4marco-bert\n",
        "if not 'dl4marco-berto' in sys.path:\n",
        "  sys.path += ['dl4marco-bert']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVrrnw6Jmj_N"
      },
      "source": [
        "**Thirdly**, prepare for training:\n",
        "\n",
        "*  Specify training data.\n",
        "*  Specify BERT pretrained model\n",
        "*  Specify GS bucket, create output directory for model checkpoints and eval results.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download Trained model\n",
        "!gsutil cp gs://luanps/monoBERT/BERT_Large_trained_on_MSMARCO.zip .\n",
        "!mkdir models/\n",
        "!mv BERT_Large_trained_on_MSMARCO.zip models/ \n",
        "!cd models/ && unzip BERT_Large_trained_on_MSMARCO.zip "
      ],
      "metadata": {
        "id": "FOkrTa3SkXMJ",
        "outputId": "b04ce03b-b3f0-4091-b3bd-c0b002bc98ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  BERT_Large_trained_on_MSMARCO.zip\n",
            "  inflating: bert_config.json        \n",
            "  inflating: model.ckpt-100000.data-00000-of-00001  \n",
            "  inflating: model.ckpt-100000.index  \n",
            "  inflating: model.ckpt-100000.meta  \n",
            "  inflating: vocab.txt               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UtHASdOmryD",
        "outputId": "3693bf0e-28a8-4b6f-fbf0-17da1fffad86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** BERT pretrained directory: models/ *****\n",
            "***** Model output directory: gs://luanps/monoBERT *****\n",
            "***** Data directory: gs://bert_msmarco_data/tfrecord *****\n"
          ]
        }
      ],
      "source": [
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_PRETRAINED_DIR = 'models/'\n",
        "#'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
        "#!gsutil ls $BERT_PRETRAINED_DIR\n",
        "\n",
        "OUTPUT_DIR = 'gs://luanps/monoBERT' #@param {type:\"string\"}\n",
        "assert OUTPUT_DIR, 'Must specify an existing GCS bucket name'\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Now we need to specify the input data dir. Should contain the .tfrecord files \n",
        "# and the supporting query-docids mapping files.\n",
        "DATA_DIR = 'gs://bert_msmarco_data/tfrecord' #@param {type:\"string\"}\n",
        "print('***** Data directory: {} *****'.format(DATA_DIR))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82qY9OMDo2Sg"
      },
      "source": [
        "**Now, we can start training/evaluating**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "S5qTdr8N6vwn"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import metrics\n",
        "import modeling\n",
        "import optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ITdDmGHQ6xil"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "USE_TPU = False\n",
        "DO_TRAIN = False  # Whether to run training.\n",
        "DO_EVAL = True  # Whether to run evaluation.\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-6\n",
        "NUM_TRAIN_STEPS = 400000\n",
        "NUM_WARMUP_STEPS = 40000\n",
        "MAX_SEQ_LENGTH = 512\n",
        "SAVE_CHECKPOINTS_STEPS = 1000\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "NUM_TPU_CORES = 8\n",
        "BERT_CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "MSMARCO_OUTPUT = True  # Write the predictions to a MS-MARCO-formatted file.\n",
        "MAX_EVAL_EXAMPLES = None  # Maximum number of examples to be evaluated.\n",
        "NUM_EVAL_DOCS = 1000  # Number of docs per query in the dev and eval files.\n",
        "METRICS_MAP = ['MAP', 'RPrec', 'NDCG', 'MRR', 'MRR@10']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ogZtwPXg62qm"
      },
      "outputs": [],
      "source": [
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  output_layer = model.get_pooled_output()\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, log_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ghWYtgK768o4"
      },
      "outputs": [],
      "source": [
        "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    (total_loss, per_example_loss, log_probs) = create_model(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "\n",
        "    scaffold_fn = None\n",
        "    initialized_variable_names = []\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "\n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\n",
        "              \"log_probs\": log_probs,\n",
        "              \"label_ids\": label_ids,\n",
        "          },\n",
        "          scaffold_fn=scaffold_fn)\n",
        "\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_tdzrGGF7Amj"
      },
      "outputs": [],
      "source": [
        "def input_fn_builder(dataset_path, seq_length, is_training,\n",
        "                     max_eval_examples=None):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "\n",
        "    batch_size = params[\"batch_size\"]\n",
        "    output_buffer_size = batch_size * 1000\n",
        "\n",
        "    def extract_fn(data_record):\n",
        "      features = {\n",
        "          \"query_ids\": tf.FixedLenSequenceFeature(\n",
        "              [], tf.int64, allow_missing=True),\n",
        "          \"doc_ids\": tf.FixedLenSequenceFeature(\n",
        "              [], tf.int64, allow_missing=True),\n",
        "          \"label\": tf.FixedLenFeature([], tf.int64),\n",
        "      }\n",
        "      sample = tf.parse_single_example(data_record, features)\n",
        "\n",
        "      query_ids = tf.cast(sample[\"query_ids\"], tf.int32)\n",
        "      doc_ids = tf.cast(sample[\"doc_ids\"], tf.int32)\n",
        "      label_ids = tf.cast(sample[\"label\"], tf.int32)\n",
        "      input_ids = tf.concat((query_ids, doc_ids), 0)\n",
        "\n",
        "      query_segment_id = tf.zeros_like(query_ids)\n",
        "      doc_segment_id = tf.ones_like(doc_ids)\n",
        "      segment_ids = tf.concat((query_segment_id, doc_segment_id), 0)\n",
        "\n",
        "      input_mask = tf.ones_like(input_ids)\n",
        "\n",
        "      features = {\n",
        "          \"input_ids\": input_ids,\n",
        "          \"segment_ids\": segment_ids,\n",
        "          \"input_mask\": input_mask,\n",
        "          \"label_ids\": label_ids,\n",
        "      }\n",
        "      return features\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset([dataset_path])\n",
        "    dataset = dataset.map(\n",
        "        extract_fn, num_parallel_calls=4).prefetch(output_buffer_size)\n",
        "\n",
        "    if is_training:\n",
        "      dataset = dataset.repeat()\n",
        "      dataset = dataset.shuffle(buffer_size=1000)\n",
        "    else:\n",
        "      if max_eval_examples:\n",
        "        # Use at most this number of examples (debugging only).\n",
        "        dataset = dataset.take(max_eval_examples)\n",
        "        # pass\n",
        "\n",
        "    dataset = dataset.padded_batch(\n",
        "        batch_size=batch_size,\n",
        "        padded_shapes={\n",
        "            \"input_ids\": [seq_length],\n",
        "            \"segment_ids\": [seq_length],\n",
        "            \"input_mask\": [seq_length],\n",
        "            \"label_ids\": [],\n",
        "        },\n",
        "        padding_values={\n",
        "            \"input_ids\": 0,\n",
        "            \"segment_ids\": 0,\n",
        "            \"input_mask\": 0,\n",
        "            \"label_ids\": 0,\n",
        "        },\n",
        "        drop_remainder=True)\n",
        "\n",
        "    return dataset\n",
        "  return input_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y_aQMUUGqMj",
        "outputId": "a73bb353-851e-4f25-c332-1df6c117d035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f30e58d4170>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://luanps/monoBERT', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f30e58cdc90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
            "INFO:tensorflow:***** Running evaluation *****\n",
            "INFO:tensorflow:  Batch size = 32\n"
          ]
        }
      ],
      "source": [
        "#def main(_):\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "if not DO_TRAIN and not DO_EVAL:\n",
        "  raise ValueError(\"At least one of `DO_TRAIN` or `DO_EVAL` must be True.\")\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG_FILE)\n",
        "\n",
        "if MAX_SEQ_LENGTH > bert_config.max_position_embeddings:\n",
        "  raise ValueError(\n",
        "      \"Cannot use sequence length %d because the BERT model \"\n",
        "      \"was only trained up to sequence length %d\" %\n",
        "      (MAX_SEQ_LENGTH, bert_config.max_position_embeddings))\n",
        "\n",
        "tpu_cluster_resolver = None\n",
        "if USE_TPU:\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
        "      TPU_ADDRESS)\n",
        "\n",
        "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=is_per_host))\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "    bert_config=bert_config,\n",
        "    num_labels=2,\n",
        "    init_checkpoint=INIT_CHECKPOINT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=NUM_TRAIN_STEPS,\n",
        "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "    use_tpu=USE_TPU,\n",
        "    use_one_hot_embeddings=USE_TPU)\n",
        "\n",
        "# If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "# or GPU.\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=USE_TPU,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    predict_batch_size=EVAL_BATCH_SIZE)\n",
        "\n",
        "if DO_TRAIN:\n",
        "  tf.logging.info(\"***** Running training *****\")\n",
        "  tf.logging.info(\"  Batch size = %d\", TRAIN_BATCH_SIZE)\n",
        "  tf.logging.info(\"  Num steps = %d\", NUM_TRAIN_STEPS)\n",
        "  train_input_fn = input_fn_builder(\n",
        "      dataset_path=DATA_DIR + \"/dataset_train.tf\",\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True)\n",
        "  estimator.train(input_fn=train_input_fn,\n",
        "                  max_steps=NUM_TRAIN_STEPS)\n",
        "  tf.logging.info(\"Done Training!\")\n",
        "\n",
        "if DO_EVAL:\n",
        "  for set_name in [\"dev\", \"eval\"]:\n",
        "    tf.logging.info(\"***** Running evaluation *****\")\n",
        "    tf.logging.info(\"  Batch size = %d\", EVAL_BATCH_SIZE)\n",
        "    max_eval_examples = None\n",
        "    if MAX_EVAL_EXAMPLES:\n",
        "      max_eval_examples = MAX_EVAL_EXAMPLES * NUM_EVAL_DOCS\n",
        "\n",
        "    eval_input_fn = input_fn_builder(\n",
        "        dataset_path=DATA_DIR + \"/dataset_\" + set_name + \".tf\",\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=False,\n",
        "        max_eval_examples=max_eval_examples)\n",
        "\n",
        "    if MSMARCO_OUTPUT:\n",
        "      msmarco_file = tf.gfile.Open(\n",
        "          OUTPUT_DIR + \"/msmarco_predictions_\" + set_name + \".tsv\", \"w\")\n",
        "      query_docids_map = []\n",
        "      with tf.gfile.Open(\n",
        "          DATA_DIR + \"/query_doc_ids_\" + set_name + \".txt\") as ref_file:\n",
        "        for line in ref_file:\n",
        "          query_docids_map.append(line.strip().split(\"\\t\"))\n",
        "\n",
        "    # ***IMPORTANT NOTE***\n",
        "    # The logging output produced by the feed queues during evaluation is very \n",
        "    # large (~14M lines for the dev set), which causes the tab to crash if you\n",
        "    # don't have enough memory on your local machine. We suppress this \n",
        "    # frequent logging by setting the verbosity to WARN during the evaluation\n",
        "    # phase.\n",
        "    tf.logging.set_verbosity(tf.logging.WARN)\n",
        "    \n",
        "    result = estimator.predict(input_fn=eval_input_fn,\n",
        "                                yield_single_examples=True)\n",
        "    start_time = time.time()\n",
        "    results = []\n",
        "    all_metrics = np.zeros(len(METRICS_MAP))\n",
        "    example_idx = 0\n",
        "    total_count = 0\n",
        "    for item in result:\n",
        "      results.append((item[\"log_probs\"], item[\"label_ids\"]))\n",
        "\n",
        "      if len(results) == NUM_EVAL_DOCS:\n",
        "\n",
        "        log_probs, labels = zip(*results)\n",
        "        log_probs = np.stack(log_probs).reshape(-1, 2)\n",
        "        labels = np.stack(labels)\n",
        "\n",
        "        scores = log_probs[:, 1]\n",
        "        pred_docs = scores.argsort()[::-1]\n",
        "        gt = set(list(np.where(labels > 0)[0]))\n",
        "\n",
        "        all_metrics += metrics.metrics(\n",
        "            gt=gt, pred=pred_docs, metrics_map=METRICS_MAP)\n",
        "\n",
        "        if MSMARCO_OUTPUT:\n",
        "          start_idx = example_idx * NUM_EVAL_DOCS\n",
        "          end_idx = (example_idx + 1) * NUM_EVAL_DOCS\n",
        "          query_ids, doc_ids = zip(*query_docids_map[start_idx:end_idx])\n",
        "          assert len(set(query_ids)) == 1, \"Query ids must be all the same.\"\n",
        "          query_id = query_ids[0]\n",
        "          rank = 1\n",
        "          for doc_idx in pred_docs:\n",
        "            doc_id = doc_ids[doc_idx]\n",
        "            # Skip fake docs, as they are only used to ensure that each query\n",
        "            # has 1000 docs.\n",
        "            if doc_id != \"00000000\":\n",
        "              msmarco_file.write(\n",
        "                  \"\\t\".join((query_id, doc_id, str(rank))) + \"\\n\")\n",
        "              rank += 1\n",
        "\n",
        "        example_idx += 1\n",
        "        results = []\n",
        "\n",
        "      total_count += 1\n",
        "\n",
        "      if total_count % 10000 == 0:\n",
        "        tf.logging.warn(\"Read {} examples in {} secs. Metrics so far:\".format(\n",
        "            total_count, int(time.time() - start_time)))\n",
        "        tf.logging.warn(\"  \".join(METRICS_MAP))\n",
        "        tf.logging.warn(all_metrics / example_idx)\n",
        "\n",
        "    # Once the feed queues are finished, we can set the verbosity back to \n",
        "    # INFO.\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "    \n",
        "    if MSMARCO_OUTPUT:\n",
        "      msmarco_file.close()\n",
        "\n",
        "    all_metrics /= example_idx\n",
        "\n",
        "    tf.logging.info(\"Eval {}:\".format(set_name))\n",
        "    tf.logging.info(\"  \".join(METRICS_MAP))\n",
        "    tf.logging.info(all_metrics)\n",
        "\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "#  tf.app.run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter(result)"
      ],
      "metadata": {
        "id": "RohEGon8ffV7",
        "outputId": "78781960-eb92-45d7-d2f7-c765f9492061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object TPUEstimator.predict at 0x7f3168d750d0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in result:\n",
        "      results.append((item[\"log_probs\"], item[\"label_ids\"]))"
      ],
      "metadata": {
        "id": "EqevY69ioHoL"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "dsu_0FCPoRE4",
        "outputId": "499081ee-4f7c-4c1b-bf33-b566e5215ecc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object TPUEstimator.predict at 0x7f3168d750d0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "monoBERT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}